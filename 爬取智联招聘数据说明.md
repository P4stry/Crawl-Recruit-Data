使用 python scrapy进行爬取智联招聘网站的招聘数据，将数据分别存储到 SQLServer和Mongodb中，以便后续使用SQLServer BI和Python进行数据分析，挖掘。

对于本次的爬取操作的相关说明：

一、      爬取前的页面分析

首先查看”智联招聘”网站首页，在首页上可以发现各职业分类的列表，如下图所示。
![分类](https://github.com/Shadow-Hunter-X/Crawl-Recruit-Data/blob/master/res/res1.jpg)

	查看网页源代码，并可以直接看到网页源代码（如下图样例），这样对于爬取操作就变得简单一些。
![源码](https://github.com/Shadow-Hunter-X/Crawl-Recruit-Data/blob/master/res/res2.jpg)
即在有源码的情况下，可以先将首页上所有的职业链接都爬取下来后存放到数据库中，后在有选择性的进行对感兴趣的职业进行爬取。

	进入到特定的职业页面上，查看页面信息（如跳转到“网站构架师”），如下图。
![样式](https://github.com/Shadow-Hunter-X/Crawl-Recruit-Data/blob/master/res/res3.jpg)

查看网页源代码，进行提取对应招聘信息列表的 Xpath或Css Selector表达式，如下图的所示。
![选择](https://github.com/Shadow-Hunter-X/Crawl-Recruit-Data/blob/master/res/res4.jpg)

详细查看列表中的每项中包含的信息，筛选提取有用信息，并在scrapy框架中的items文件中指定对应的存储结构体。

	在爬取完一页后，需要跳转到下一页
由于在每种类型下的工作岗位数不同，所以需要在爬取前，先将需要爬取的页数读取出。
![分页](https://github.com/Shadow-Hunter-X/Crawl-Recruit-Data/blob/master/res/res4.jpg)

